{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(1,\"../Simulation_Pybullet/\")\n",
    "from KinovaGen3Class import KinovaGen3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device  cpu\n",
      "models/urdf/JACO3_URDF_V11.urdf\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Not connected to physics server.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c5ae51ed5670>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#Create a experiment env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrobot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKinovaGen3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#Initially parameters of the urdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/Simulation_Pybullet/KinovaGen3Class.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, urdf_root, root, robot_urdf, robot_launch_pos, robot_launch_orien, last_robot_joint_name, robot_control_joints, robot_mimic_joints_name, robot_mimic_joints_master, robot_mimic_multiplier, tool_orient_e, nullspace, home_angles, visual_inspection, tcp_offset_pos, tcp_offset_orien_e, time_step)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mtcp_offset_orien_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtcp_offset_orien_e\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         time_step=time_step)\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/Simulation_Pybullet/RobotClassDev.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, urdf_root, root, robot_urdf, robot_launch_pos, robot_launch_orien, last_robot_joint_name, robot_control_joints, robot_mimic_joints_name, robot_mimic_joints_master, robot_mimic_multiplier, tool_orient_e, nullspace, home_angles, visual_inspection, tcp_offset_pos, tcp_offset_orien_e, save_database, database_name, time_step)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# launch robot in the world\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobot_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadURDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrobot_urdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrobot_launch_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrobot_launch_orien\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURDF_USE_SELF_COLLISION_EXCLUDE_PARENT\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Robot launched\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetTimeStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: Not connected to physics server."
     ]
    }
   ],
   "source": [
    "#To improve the velocity, run it on the GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device ', device)\n",
    "\n",
    "\n",
    "\n",
    "#Create a experiment env\n",
    "robot = KinovaGen3()\n",
    "\n",
    "#Initially parameters of the urdf\n",
    "\n",
    "\n",
    "print('observation space:', env.observation_space) #states, There is only 1 state constant\n",
    "print('action space:', env.action_space) #parameters, number of parameters choose to tune, continuous\n",
    "\n",
    "#t.sleep(10)\n",
    "\n",
    "\n",
    "#Creation of a class to choose the parameters, the policy\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, env, h_size=16):\n",
    "        super(Agent, self).__init__() #Means that this class heritage the init from nn.Module class\n",
    "                                      # nn.Module it's the base class for all the neural net networks\n",
    "                                      # https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module\n",
    "                                      # https://pytorch.org/docs/stable/nn.html\n",
    "        \n",
    "        self.env = env #Save the enviorment \n",
    "        \n",
    "        #Dimensions constant = 1, due to the aren't inputs, hidden layer, parameters sizes\n",
    "        self.s_size = env.observation_space.shape[0] #First layer 1 as there is only 1 state\n",
    "        self.h_size = h_size #hidden layer\n",
    "        self.a_size = env.action_space.shape[0] #Last layer number of parameters to tune, in tant per 1 to reach faster results\n",
    "        \n",
    "        # define layers\n",
    "        self.fc1 = nn.Linear(self.s_size, self.h_size) # A linear layer that connect the states with the hidden layer\n",
    "        self.fc2 = nn.Linear(self.h_size, self.a_size) # Hidden layer the from hidden layer to actions\n",
    "        \n",
    "    def set_weights(self, weights):\n",
    "        s_size = self.s_size\n",
    "        h_size = self.h_size\n",
    "        a_size = self.a_size\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "        # Are linear layers so \n",
    "        # weight _w and b it's the bias.\n",
    "        #https://medium.com/datathings/linear-layers-explained-in-a-simple-way-2319a9c2d1aa\n",
    "        \n",
    "        \n",
    "        #The bias learns a constant value, independent of the input.\n",
    "        \n",
    "        # Learns that all the positive states need at least a bias constant + something dependant\n",
    "        \n",
    "        # a linear layer learns that the output ,it's the Activation Function ( input * pendent (the weight) + constant)\n",
    "        \n",
    "        # linear neuron output = input *w + b\n",
    "        \n",
    "        # separate the weights for each layer\n",
    "        \n",
    "        # so we are saying that (state1 * wl1 + bl1)*wl2 +bl2, so we belive it follows a 1st order equation * activation function\n",
    "        \n",
    "        \n",
    "        fc1_end = (s_size*h_size)+h_size \n",
    "        #The first states * number of hidden layers are the weights of the first layer, each network has different weights for each state input\n",
    "        fc1_W = torch.from_numpy(weights[:s_size*h_size].reshape(s_size, h_size))\n",
    "        #From the previous end , the follwing hidden layer neurons number weights are the bias, each neuron has only 1 bias, doesn't depend on the state input\n",
    "        fc1_b = torch.from_numpy(weights[s_size*h_size:fc1_end])\n",
    "        \n",
    "        #Every neuron has a weight for each action output\n",
    "        fc2_W = torch.from_numpy(weights[fc1_end:fc1_end+(h_size*a_size)].reshape(h_size, a_size))\n",
    "        fc2_b = torch.from_numpy(weights[fc1_end+(h_size*a_size):])\n",
    "       \n",
    "        # set the weights for each layer\n",
    "        self.fc1.weight.data.copy_(fc1_W.view_as(self.fc1.weight.data))\n",
    "        self.fc1.bias.data.copy_(fc1_b.view_as(self.fc1.bias.data))\n",
    "        self.fc2.weight.data.copy_(fc2_W.view_as(self.fc2.weight.data))\n",
    "        self.fc2.bias.data.copy_(fc2_b.view_as(self.fc2.bias.data))\n",
    "    \n",
    "    def get_weights_dim(self):\n",
    "        #In reality its returning the weights + bias dimensions, the +1 its the bias\n",
    "        return (self.s_size+1)*self.h_size + (self.h_size+1)*self.a_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #forward its Method to be able to pass the data as batches of data\n",
    "        # It passes the data as Matrices adding the activation functions at the same time\n",
    "        \n",
    "        #They have activation functions to\n",
    "        \n",
    "        #https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n",
    "        \n",
    "        x = F.relu(self.fc1(x)) # Only possitive values pass\n",
    "        #x = F.tanh(self.fc2(x))\n",
    "        x = torch.tanh(self.fc2(x)) # Only from -1 to 1 values, as the action are \n",
    "                                    #go back , stay or go forward\n",
    "                                    #it's perfect for the last layer\n",
    "        return x.cpu().data\n",
    "        \n",
    "    def evaluate(self, weights, gamma=1.0, max_t=5000):\n",
    "        # Obtain the accumulative reward from the actions selected by the neural net\n",
    "        self.set_weights(weights)\n",
    "        episode_return = 0.0\n",
    "        state = self.env.reset()\n",
    "        for t in range(max_t):\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "            action = self.forward(state)\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            episode_return += reward * math.pow(gamma, t)\n",
    "            if done:\n",
    "                break\n",
    "        return episode_return\n",
    "#End of class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print( env.observation_space.shape[0] ) #First layer number of states+\n",
    "h_size=16\n",
    "print( env.action_space.shape[0] ) #hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env).to(device) # Creation of a neural net in the device, in my case the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Entrophy Method, to choose the weights\n",
    "\n",
    "# In my case where only 1 action,and that it's apply the parameters do another step doesn't change anything due to the state doesn't vary\n",
    "# For this reason max_t and gama doesn't make sense, so I set them to max_t = 1 and gamma to 0\n",
    "def cem(n_iterations=500, max_t=1, gamma=0.0, print_every=500*0.1, pop_size=50, elite_frac=0.2, sigma=0.5):\n",
    "    \"\"\"PyTorch implementation of the cross-entropy method.\n",
    "        \n",
    "    Params\n",
    "    ======\n",
    "        n_iterations (int): maximum number of training iterations\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        gamma (float): discount rate\n",
    "        print_every (int): how often to print average score (over last 100 episodes)\n",
    "        pop_size (int): size of population at each iteration\n",
    "        elite_frac (float): percentage of top performers to use in update\n",
    "        sigma (float): standard deviation of additive noise\n",
    "    \"\"\"\n",
    "    #Numbers of elements that you keep as the better ones\n",
    "    n_elite=int(pop_size*elite_frac)\n",
    "    \n",
    "    #scores doble end queee , from iterations size * 0.1\n",
    "    scores_deque = deque(maxlen=n_iterations*0.1)\n",
    "    #intial scores empty\n",
    "    scores = []\n",
    "    \n",
    "    #Initial best weights, are from 0 to 1, it's good to be small the weights, but they should be different from 0.\n",
    "    # small to avoid overfiting , different from 0 to update them\n",
    "    \n",
    "    best_weight = sigma*np.random.randn(agent.get_weights_dim())\n",
    "\n",
    "    #Each iteration, modify  + (from 0 to 1) the best weight randomly\n",
    "    #Computes the reward with these weights\n",
    "    #Sort the reward to get the best ones\n",
    "    # Save the best weights\n",
    "    # the Best weight it's the mean of the best one\n",
    "    #compute the main reward of the main best rewards ones\n",
    "    #this it's show to evalute how good its\n",
    "    \n",
    "    for i_iteration in range(1, n_iterations+1):\n",
    "        \n",
    "        #Generate new population weights, as a mutation of the best weight to test them\n",
    "        weights_pop = [best_weight + (sigma*np.random.randn(agent.get_weights_dim())) for i in range(pop_size)]\n",
    "        #Compute the parameters and obtain the rewards for each of them\n",
    "        rewards = np.array([agent.evaluate(weights, gamma, max_t) for weights in weights_pop])\n",
    "        \n",
    "        #Sort the rewards to obtain the best ones\n",
    "        elite_idxs = rewards.argsort()[-n_elite:]\n",
    "        elite_weights = [weights_pop[i] for i in elite_idxs]\n",
    "        \n",
    "        #Set the best weight as the mean of the best ones \n",
    "        best_weight = np.array(elite_weights).mean(axis=0)\n",
    "        \n",
    "        #Get the reward with this new weight\n",
    "        reward = agent.evaluate(best_weight, gamma=gamma)\n",
    "        scores_deque.append(reward)\n",
    "        scores.append(reward)\n",
    "        \n",
    "        #save the check point\n",
    "        torch.save(agent.state_dict(), 'checkpoint.pth')\n",
    "        \n",
    "        if i_iteration % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_iteration, np.mean(scores_deque)))\n",
    "\n",
    "        if np.mean(scores_deque)>=0.0:\n",
    "            print('\\nEnvironment solved in {:d} iterations!\\tAverage Score: {:.2f}'.format(i_iteration-n_iterations*0.1, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execute the cross entrophy method with default Values\n",
    "#scores = cem()\n",
    "\n",
    "\n",
    "#To don't ask the GPU as much reduce the pop_size, it's the amount of elemts try\n",
    "scores = cem(pop_size=30)\n",
    "# \n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights from file\n",
    "agent.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "state = env.reset()\n",
    "while True:\n",
    "    state = torch.from_numpy(state).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        action = agent(state)\n",
    "    env.render()\n",
    "    t.sleep(0.02)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
