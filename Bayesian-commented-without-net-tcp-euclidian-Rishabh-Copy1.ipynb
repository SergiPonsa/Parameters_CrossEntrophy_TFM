{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import GPy\n",
    "\n",
    "import time as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from env_pybullet_gen3 import env_pybullet_kin_gen3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola\n",
      "../Simulation_Pybullet/models/urdf/JACO3_URDF_V11.urdf\n",
      "Robot launched\n",
      "hola\n",
      "(7, 12)\n",
      "(7, 17)\n",
      "0    168\n",
      "1    151\n",
      "2    128\n",
      "3     94\n",
      "4    210\n",
      "5     48\n",
      "6    189\n",
      "Name: max_vel, dtype: int64\n",
      "       mass damping       Ixx       Iyy       Izz   kp   ki   kd  max_vel  \\\n",
      "0  1.377353       0  0.004801  0.004755  0.002283  0.1  0.0  0.0      168   \n",
      "1  1.163667       0  0.008419  0.001920  0.008361  0.1  0.0  0.0      151   \n",
      "2  1.163660       0  0.007545  0.007487  0.001921  0.1  0.0  0.0      128   \n",
      "3  0.930287       0  0.006410  0.001380  0.006518  0.1  0.0  0.0       94   \n",
      "4  0.678106       0  0.001680  0.001506  0.000826  0.1  0.0  0.0      210   \n",
      "5  0.678106       0  0.001938  0.000827  0.001763  0.1  0.0  0.0       48   \n",
      "6  0.500657       0  0.000775  0.000585  0.000975  0.1  0.0  0.0      189   \n",
      "\n",
      "   force_x_one  \n",
      "0            1  \n",
      "1            1  \n",
      "2            1  \n",
      "3            1  \n",
      "4            1  \n",
      "5            1  \n",
      "6            1  \n",
      "observation space: 1\n",
      "mass okey\n",
      "max_vel okey\n",
      "kp okey\n",
      "ki okey\n",
      "kd okey\n",
      "force_x_one okey\n",
      "Ixx okey\n",
      "Iyy okey\n",
      "Izz okey\n",
      "action space: 63\n",
      "original action: [1.377353, 1.163667, 1.16366, 0.930287, 0.678106, 0.678106, 0.500657, 168, 151, 128, 94, 210, 48, 189, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 1, 1, 1, 1, 1, 1, 1, 0.00480078220558528, 0.008418724123621643, 0.007544516197001279, 0.0064096919604697605, 0.0016797846804208455, 0.0019375935324615593, 0.0007750385804833523, 0.004755191268457921, 0.0019202057294098781, 0.007486605057526543, 0.0013804130332615912, 0.0015062421641084327, 0.0008273237988932355, 0.0005849825981943527, 0.0022826303695446856, 0.00836116845951151, 0.0019205500000651847, 0.006517816917001926, 0.0008260694053789821, 0.0017630597813546379, 0.0009751695712112207]\n"
     ]
    }
   ],
   "source": [
    "#Create a experiment env\n",
    "env = env_pybullet_kin_gen3(no_zeros = True,Excel_path_Okay_tcp = \"./positions_from_joints_19.xlsx\",time_step=0.05,home_angles = [-0.207226464676801,1.5689993219813,-1.01515387451347,-2.45271819663908,2.00795352004673,1.91098991659003,-0.831045149646278])\n",
    "env.robot.visual_inspection = True\n",
    "\n",
    "#Initially parameters of the urdf\n",
    "\n",
    "#Make maxvels closer to the reality\n",
    "#both have to be modified\n",
    "env.max_vel = [168,151,128,94,210,48,189]\n",
    "env.original_parameters_df [\"max_vel\"]=env.max_vel\n",
    "print(env.original_parameters_df[\"max_vel\"])\n",
    "print(env.original_parameters_df)\n",
    "\n",
    "\n",
    "print('observation space:', env.observation_space) #states, There is only 1 state constant\n",
    "env.update_parameters_to_modify([\"mass\",\"max_vel\",\"kp\",\"ki\",\"kd\",\"force_x_one\",\"Ixx\",\"Iyy\",\"Izz\"])\n",
    "print('action space:', env.action_space) #parameters, number of parameters choose to tune, continuous\n",
    "print('original action:', env.action_original()) #parameters, number of parameters choose to tune, continuous\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a first search\n",
    "pop_size = 70*20\n",
    "sigma = 0.0001\n",
    "original_action = np.array(env.action_original())\n",
    "np.random.seed(0)\n",
    "Create = True\n",
    "rewards = []\n",
    "actions = []\n",
    "\n",
    "if(Create ==True):\n",
    "    #Generate new population weights to test\n",
    "    weights_pop = [(sigma*np.random.randn(env.action_space)) for i in range(pop_size)]\n",
    "    for weights in weights_pop:\n",
    "                    action=np.add(np.multiply(weights,original_action),original_action)\n",
    "                    actions.append(action)\n",
    "                    rewards.append(env.step_tcp_rishabh(action))\n",
    "    actions = np.array(actions)\n",
    "    rewards = np.array(rewards).reshape(actions.shape[0],1)\n",
    "    \n",
    "    i_data = np.hstack((actions,rewards))\n",
    "    np.save(\"i_data.npy\",i_data)\n",
    "else:\n",
    "    i_data = np.load(\"i_data.npy\")\n",
    "    actions = i_data[:,:-1]\n",
    "    rewards = i_data[:,-1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"actions\")\n",
    "print(actions.shape)\n",
    "print(actions)\n",
    "print(\"rewards\")\n",
    "print(rewards.shape)\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Gaussian model, which models the relation between parameters and score\n",
    "kernel = GPy.kern.Linear(actions.shape[1], ARD=1)\n",
    "#kernel = GPy.kern.RBF_inv(X.shape[1], ARD=1)\n",
    "\n",
    "model = GPy.models.GPRegression(actions, rewards, kernel)\n",
    "\n",
    "model.optimize(optimizer='scg', max_iters=10**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =  model.predict(actions)\n",
    "\n",
    "error = abs(np.array(predictions[0])-rewards)\n",
    "error_max = max(error)\n",
    "error_mean = error.mean(axis=0)\n",
    "\n",
    "print(\"error\")\n",
    "print(error)\n",
    "print(\"error_max\")\n",
    "print(error_max)\n",
    "print(\"error_mean\")\n",
    "print(error_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best action from regressed model\n",
    "\n",
    "def Get_best_action_from_model(env,model,sigma = 0.5, population = 70*10**3,n_elite = 10):\n",
    "    if(population>70*10**3):\n",
    "        print(\"More data fill the memory, and creates and error, used only 70*10**3\")\n",
    "        population = 70*10**3\n",
    "    \n",
    "    weights_pop_model = [(sigma*np.random.randn(env.action_space)) for i in range(population)]\n",
    "    \n",
    "    actions_model = []\n",
    "    for weights in weights_pop_model:\n",
    "        action=np.add(np.multiply(weights,original_action),original_action)\n",
    "        actions_model.append(action)\n",
    "    actions_model = np.array(actions_model)\n",
    "    \n",
    "    prediction = model.predict(actions_model)\n",
    "    rewards_model = np.array(prediction[0])\n",
    "    \n",
    "    elite_idxs = rewards_model.argsort()[-n_elite:]\n",
    "    elite_actions = [actions_model[i] for i in elite_idxs]\n",
    "    elite_rewards = [rewards_model[i] for i in elite_idxs]\n",
    "\n",
    "    #Set the best weight as the mean of the best ones \n",
    "\n",
    "    best_action = np.array(elite_actions).mean(axis=0)\n",
    "    best_rewards = rewards_model.argsort()[-n_elite:]\n",
    "    \n",
    "    return np.array(elite_actions),np.array(elite_rewards)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_action,best_rewards = Get_best_action_from_model(env,model)\n",
    "print(best_rewards)\n",
    "print(best_rewards.shape)\n",
    "print(best_action)\n",
    "print(best_action.shape)\n",
    "\n",
    "#Look which x provide an Y 0\n",
    "print(rewards)\n",
    "desired_error_list = []\n",
    "desired_error = 0\n",
    "for i in range(rewards.shape[0]):\n",
    "    desired_error_list.append([desired_error])\n",
    "desired_erro_np = np.array(desired_error_list)\n",
    "print(desired_erro_np )\n",
    "predicted_actions = model.infer_newX(desired_erro_np , optimize=False)\n",
    "predicted_actions = np.array(predicted_actions[0])\n",
    "print(predicted_actions.shape)\n",
    "print(predicted_actions[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian Search\n",
    "def bayesian_learn(env,model, pop_size=env.action_space, sigma=0.3):\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        best_action_m,best_rewards_m = Get_best_action_from_model(env,model)\n",
    "        \n",
    "        \n",
    "        weights_pop_model = [(sigma*np.random.randn(env.action_space)) for i in range(pop_size)]\n",
    "        \n",
    "        rewards_explore = []\n",
    "        actions_explore = []\n",
    "        for weights in weights_pop_model:\n",
    "            action=np.add(np.multiply(weights,best_action_m),best_action_m)\n",
    "            actions_explore.append(action)\n",
    "            rewards_explore.append(env.step_tcp_rishabh(action))\n",
    "        \n",
    "        return actions_explore,rewards_explore\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_inputs = range(env.action_space)\n",
    "\n",
    "#decide the free input\n",
    "free_input = fixed_inputs.pop(0)\n",
    "model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.objective_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Execute the cross entrophy method with default Values\n",
    "#scores = cem()\n",
    "\n",
    "\n",
    "#To don't ask the GPU as much reduce the pop_size, it's the amount of elemts try\n",
    "scores,best_actions = cem_no_net()\n",
    "# \n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot the last scores zoom\n",
    "fig = plt.figure()\n",
    "zoom= 300\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, zoom+1), scores[-zoom:])\n",
    "plt.ylabel('Score zoom')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot actions\n",
    "best_actions_np = np.array(best_actions)\n",
    "joint = 1\n",
    "for i in range(len(env.parameters_to_modify)) :\n",
    "    parameter = env.parameters_to_modify[i]\n",
    "    figures = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(1, best_actions_np.shape[0]+1), best_actions_np[:,joint+i*7])\n",
    "    plt.ylabel(parameter+\" Joint\"+str(joint))\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the weights from file\n",
    "# Not working know\n",
    "\n",
    "\n",
    "#state = env.reset()\n",
    "env = env_pybullet_kin_gen3(no_zeros = True,Excel_path_Okay_tcp = \"./positions_from_joints_19.xlsx\",time_step=0.05,home_angles = [-0.207226464676801,1.5689993219813,-1.01515387451347,-2.45271819663908,2.00795352004673,1.91098991659003,-0.831045149646278])\n",
    "env.robot.visual_inspection = False\n",
    "\n",
    "#Make maxvels closer to the reality\n",
    "env.max_vel = [168,151,128,94,210,48,189]\n",
    "env.original_parameters_df [\"max_vel\"]=env.max_vel\n",
    "env.modified_parameters_df [\"max_vel\"]=[168,151,128,94,210,48,189]\n",
    "\n",
    "env.update_parameters_to_modify([\"mass\",\"max_vel\",\"kp\",\"ki\",\"kd\",\"force_x_one\",\"Ixx\",\"Iyy\",\"Izz\",\"damping\"])\n",
    "env.robot.visual_inspection = False\n",
    "env.modified_parameters_df = env.create_df_from_Excel(\"./Parameters_train_tcp_euc_rishabh.xlsx\")\n",
    "\n",
    "\n",
    "t.sleep(0.02)\n",
    "action = env.action_modified()\n",
    "action = np.array(action)\n",
    "print('original action:', env.action_original()) #parameters, number of parameters choose to tune, continuous\n",
    "print(\"trained\",action)\n",
    "reward = env.step_tcp_rishabh(action)\n",
    "print(\"reward\")\n",
    "print(reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to excel\n",
    "a = env.df_avg.to_numpy()\n",
    "print(a[:,5])\n",
    "env.df_avg.to_excel(\"./Train_parameters_result_tcp_euc_rishabh.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.original_parameters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the weights from file\n",
    "# Not working know\n",
    "\n",
    "\n",
    "#state = env.reset()\n",
    "env = env_pybullet_kin_gen3(no_zeros = True,Excel_path_Okay_tcp = \"./positions_from_joints_19.xlsx\",time_step=0.05,home_angles = [-0.207226464676801,1.5689993219813,-1.01515387451347,-2.45271819663908,2.00795352004673,1.91098991659003,-0.831045149646278])\n",
    "env.robot.visual_inspection = False\n",
    "\n",
    "#Make maxvels closer to the reality\n",
    "env.max_vel = [168,151,128,94,210,48,189]\n",
    "env.original_parameters_df [\"max_vel\"]=env.max_vel\n",
    "env.modified_parameters_df [\"max_vel\"]=[168,151,128,94,210,48,189]\n",
    "\n",
    "env.update_parameters_to_modify([\"mass\",\"max_vel\",\"kp\",\"ki\",\"kd\",\"force_x_one\",\"Ixx\",\"Iyy\",\"Izz\"])\n",
    "env.robot.visual_inspection = False\n",
    "env.modified_parameters_df = env.original_parameters_df\n",
    "\n",
    "\n",
    "t.sleep(0.02)\n",
    "action = env.action_modified()\n",
    "action = np.array(action)\n",
    "print(action)\n",
    "reward = env.step_tcp_rishabh(action)\n",
    "\n",
    "print(\"reward\")\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to excel\n",
    "a = env.df_avg.to_numpy()\n",
    "print(a[:,5])\n",
    "env.df_avg.to_excel(\"./Original_parameters_result_tcp_euc_rishabh.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
