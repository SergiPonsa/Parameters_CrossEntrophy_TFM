{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from env_pybullet_gen3 import env_pybullet_kin_gen3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device  cpu\n",
      "hola\n",
      "../Simulation_Pybullet/models/urdf/JACO3_URDF_V11.urdf\n",
      "Robot launched\n",
      "hola\n",
      "(7, 12)\n",
      "(7, 17)\n",
      "observation space: 1\n",
      "mass okey\n",
      "max_vel okey\n",
      "kp okey\n",
      "ki okey\n",
      "damping okey\n",
      "force_x_one okey\n",
      "Ixx okey\n",
      "Iyy okey\n",
      "Izz okey\n",
      "action space: 63\n",
      "original action: [1.377353, 1.163667, 1.16366, 0.930287, 0.678106, 0.678106, 0.500657, 30, 30, 30, 30, 30, 30, 30, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 1, 1, 1, 1, 1, 1, 1, 0.00480078220558528, 0.008418724123621643, 0.007544516197001279, 0.0064096919604697605, 0.0016797846804208455, 0.0019375935324615593, 0.0007750385804833523, 0.004755191268457921, 0.0019202057294098781, 0.007486605057526543, 0.0013804130332615912, 0.0015062421641084327, 0.0008273237988932355, 0.0005849825981943527, 0.0022826303695446856, 0.00836116845951151, 0.0019205500000651847, 0.006517816917001926, 0.0008260694053789821, 0.0017630597813546379, 0.0009751695712112207]\n"
     ]
    }
   ],
   "source": [
    "#To improve the velocity, run it on the GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device ', device)\n",
    "\n",
    "\n",
    "\n",
    "#Create a experiment env\n",
    "env = env_pybullet_kin_gen3(no_zeros = True,Excel_path_Okay_tcp = \"./test_tcp_19_averageconverted.xlsx\",time_step=0.005)\n",
    "env.robot.visual_inspection = False\n",
    "\n",
    "#Initially parameters of the urdf\n",
    "\n",
    "\n",
    "\n",
    "print('observation space:', env.observation_space) #states, There is only 1 state constant\n",
    "env.update_parameters_to_modify([\"mass\",\"max_vel\",\"kp\",\"ki\",\"damping\",\"force_x_one\",\"Ixx\",\"Iyy\",\"Izz\"])\n",
    "print('action space:', env.action_space) #parameters, number of parameters choose to tune, continuous\n",
    "print('original action:', env.action_original()) #parameters, number of parameters choose to tune, continuous\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Entrophy Method, to choose the weights\n",
    "\n",
    "# In my case where only 1 action,and that it's apply the parameters do another step doesn't change anything due to the state doesn't vary\n",
    "# For this reason max_t and gama doesn't make sense, so I set them to max_t = 1 and gamma to 0\n",
    "def cem_no_net(n_iterations=600, max_t=1, gamma=0.0, print_every=100, pop_size=env.action_space, elite_frac=0.1, sigma=0.05,sigma_reduction_every_print = 0.65, per_one = True ):\n",
    "    \"\"\"PyTorch implementation of the cross-entropy method.\n",
    "        \n",
    "    Params\n",
    "    ======\n",
    "        n_iterations (int): maximum number of training iterations\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        gamma (float): discount rate\n",
    "        print_every (int): how often to print average score (over last 100 episodes)\n",
    "        pop_size (int): size of population at each iteration\n",
    "        elite_frac (float): percentage of top performers to use in update\n",
    "        sigma (float): standard deviation of additive noise\n",
    "        per_one (boolean): to determine if the output is in per one or not\n",
    "    \"\"\"\n",
    "    #Numbers of elements that you keep as the better ones\n",
    "    n_elite=int(pop_size*elite_frac)\n",
    "    \n",
    "    #scores doble end queee , from iterations size * 0.1\n",
    "    scores_deque = deque(maxlen=int(n_iterations*0.1))\n",
    "    #intial scores empty\n",
    "    scores = []\n",
    "    #Select a seed to make the results the same every test, not depending on the seed\n",
    "    np.random.seed(0)\n",
    "    #Initial best weights, are from 0 to 1, it's good to be small the weights, but they should be different from 0.\n",
    "    # small to avoid overfiting , different from 0 to update them\n",
    "    \n",
    "    if (per_one == True):\n",
    "        best_weight = sigma*np.random.randn(env.action_space)\n",
    "        original_action = np.array(env.action_original())\n",
    "    else:\n",
    "        best_weight = np.add(sigma*np.random.randn(env.action_space),env.action_original())\n",
    "\n",
    "    #Each iteration, modify  + (from 0 to 1) the best weight randomly\n",
    "    #Computes the reward with these weights\n",
    "    #Sort the reward to get the best ones\n",
    "    # Save the best weights\n",
    "    # the Best weight it's the mean of the best one\n",
    "    #compute the main reward of the main best rewards ones\n",
    "    #this it's show to evalute how good its\n",
    "    \n",
    "    for i_iteration in range(1, n_iterations+1):\n",
    "        \n",
    "        #Generate new population weights, as a mutation of the best weight to test them\n",
    "        weights_pop = [best_weight + (sigma*np.random.randn(env.action_space)) for i in range(pop_size)]\n",
    "        \n",
    "        #Compute the parameters and obtain the rewards for each of them\n",
    "        #print(\"iteration \"+str(i_iteration))\n",
    "        if (per_one == True):\n",
    "            rewards=[]\n",
    "            for weights in weights_pop:\n",
    "                #print(\"New weights\")\n",
    "                action=np.add(np.multiply(weights,original_action),original_action)\n",
    "                #t.sleep(10)\n",
    "                rewards.append( env.step_tcp_rishabh(action) )\n",
    "            rewards = np.array(rewards)\n",
    "        else:\n",
    "            rewards=[]\n",
    "            for weights in weights_pop:\n",
    "                rewards.append(env.step_tcp_rishabh(weights) )\n",
    "            rewards = np.array(rewards)\n",
    "        print(\"rewards\" + str(i_iteration))\n",
    "        print(rewards)\n",
    "        #print(\"\\n\")\n",
    "        \n",
    "        #Sort the rewards to obtain the best ones\n",
    "        elite_idxs = rewards.argsort()[-n_elite:]\n",
    "        elite_weights = [weights_pop[i] for i in elite_idxs]\n",
    "        \n",
    "        #Set the best weight as the mean of the best ones \n",
    "       \n",
    "        best_weight = np.array(elite_weights).mean(axis=0)\n",
    "        \n",
    "        #Get the reward with this new weight\n",
    "        \n",
    "        if (per_one == True):\n",
    "            action = np.add(np.multiply(best_weight,original_action),original_action)\n",
    "            reward = env.step_tcp_rishabh(action)\n",
    "            print(reward)\n",
    "        else:\n",
    "            reward = env.step_tcp_rishabh(best_weight)\n",
    "        scores_deque.append(reward)\n",
    "        scores.append(reward)\n",
    "        \n",
    "        #save the check point\n",
    "        env.save_parameters(\"./Parameters_train_tcp_euc.xlsx\")\n",
    "        \n",
    "        if i_iteration % print_every == 0:\n",
    "            sigma = sigma * sigma_reduction_every_print\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_iteration, np.mean(scores_deque)))\n",
    "\n",
    "        if np.mean(scores_deque)>=0.0:\n",
    "            print('\\nEnvironment solved in {:d} iterations!\\tAverage Score: {:.2f}'.format(i_iteration-n_iterations*0.1, np.mean(scores_deque)))\n",
    "            break\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards1\n",
      "[-0.67096293 -0.67092371 -0.67083827 -0.67076827 -0.67068258 -0.67116003\n",
      " -0.6709193  -0.67066202 -0.67107155 -0.67101908 -0.67074551 -0.67084049\n",
      " -0.6709379  -0.67108836 -0.67092862 -0.67101246 -0.6709995  -0.67095842\n",
      " -0.67093651 -0.67086298 -0.67106051 -0.6707569  -0.6709012  -0.67079469\n",
      " -0.67107026 -0.67078829 -0.6708888  -0.67090774 -0.67098314 -0.67085242\n",
      " -0.67090045 -0.67090185 -0.6708172  -0.67095829 -0.67072842 -0.67104207\n",
      " -0.67083438 -0.67075394 -0.671175   -0.670783   -0.67103044 -0.67095615\n",
      " -0.67106148 -0.67076434 -0.67084    -0.67071577 -0.67084998 -0.67076546\n",
      " -0.67087354 -0.67091272 -0.67085882 -0.67067797 -0.67070526 -0.67094305\n",
      " -0.67086019 -0.67081095 -0.67075315 -0.67081706 -0.67074611 -0.67102152\n",
      " -0.67107574 -0.67080381 -0.67070105]\n",
      "-0.6706911610691463\n",
      "rewards2\n",
      "[-0.67054525 -0.67065657 -0.67081871 -0.67077826 -0.67091624 -0.67052008\n",
      " -0.67071004 -0.67072301 -0.67088015 -0.67067644 -0.67064789 -0.67074992\n",
      " -0.67068531 -0.67073783 -0.67060186 -0.67074926 -0.67099973 -0.67066932\n",
      " -0.67062536 -0.67078831 -0.67089896 -0.6707439  -0.67066135 -0.67074482\n",
      " -0.67074466 -0.67063379 -0.67095045 -0.67076845 -0.67052026 -0.67070038\n",
      " -0.67075617 -0.6705651  -0.67063758 -0.67065936 -0.67072561 -0.67040091\n",
      " -0.67070235 -0.67063336 -0.67048746 -0.67059968 -0.67071638 -0.67063251\n",
      " -0.67054177 -0.67058808 -0.6707048  -0.67059509 -0.67077188 -0.67097605\n",
      " -0.67081074 -0.67074783 -0.67075249 -0.67069895 -0.67066799 -0.67066646\n",
      " -0.67070854 -0.67092236 -0.67054835 -0.67073889 -0.67061696 -0.67077436\n",
      " -0.67065356 -0.67079986 -0.67075808]\n",
      "-0.6705030637508791\n",
      "rewards3\n",
      "[-0.67065595 -0.67050242 -0.67061377 -0.67050177 -0.67046918 -0.67076562\n",
      " -0.67060452 -0.67077816 -0.67048747 -0.67054669 -0.67055245 -0.67047759\n",
      " -0.67050368 -0.67055175 -0.67033158 -0.67045377 -0.67039399 -0.67071118\n",
      " -0.67066496 -0.67048243 -0.67052988 -0.67050983 -0.6702534  -0.67059804\n",
      " -0.67040046 -0.67054713 -0.67042551 -0.67053894 -0.67060859 -0.67041004\n",
      " -0.67068257 -0.6704991  -0.67036243 -0.67054716 -0.67032557 -0.67061231\n",
      " -0.67050801 -0.6705313  -0.67047555 -0.67033619 -0.67038116 -0.67050361\n",
      " -0.67050831 -0.67059639 -0.67049984 -0.67069492 -0.67060601 -0.67033908\n",
      " -0.67050521 -0.67061789 -0.67057238 -0.67052856 -0.67056468 -0.67058092\n",
      " -0.67058949 -0.67038028 -0.67041672 -0.67025077 -0.67036544 -0.67061769\n",
      " -0.67054929 -0.67053255 -0.67053888]\n",
      "-0.6703056754424186\n",
      "rewards4\n",
      "[-0.67021725 -0.67033109 -0.67002428 -0.67049061 -0.67015095 -0.67054438\n",
      " -0.67033677 -0.67023811 -0.67041317 -0.67026267 -0.67034721 -0.6702761\n",
      " -0.67025205 -0.67025902 -0.67012479 -0.67023246 -0.67031397 -0.67015334\n",
      " -0.66997358 -0.67031174 -0.67033165 -0.67013315 -0.67039875 -0.67028818\n",
      " -0.67040591 -0.67038985 -0.67023368 -0.67005298 -0.67021396 -0.6702391\n",
      " -0.67030803 -0.67046557 -0.67034243 -0.67036071 -0.6703247  -0.67019043\n",
      " -0.67035687 -0.67039927 -0.67021663 -0.67041675 -0.6702629  -0.67035713\n",
      " -0.67036397 -0.6704276  -0.67034846 -0.67034576 -0.67023576 -0.67015273\n",
      " -0.67032329 -0.67052094 -0.67030315 -0.67026936 -0.67027166 -0.6703344\n",
      " -0.67028933 -0.6700952  -0.67022426 -0.67038788 -0.6703635  -0.67029583\n",
      " -0.670387   -0.67039738 -0.67020185]\n",
      "-0.6700664443583273\n",
      "rewards5\n",
      "[-0.67005704 -0.67005027 -0.67010366 -0.6701865  -0.67023276 -0.67005875\n",
      " -0.67020432 -0.67009743 -0.67011612 -0.67006434 -0.67007843 -0.66994885\n",
      " -0.6701032  -0.66993107 -0.67004781 -0.67018578 -0.67008859 -0.67027161\n",
      " -0.67003834 -0.67006912 -0.67026396 -0.67015708 -0.6700304  -0.67008313\n",
      " -0.67022569 -0.67019086 -0.67002914 -0.67018237 -0.6700781  -0.67010564\n",
      " -0.66991758 -0.66999827 -0.67011609 -0.67007689 -0.67015922 -0.66998032\n",
      " -0.67001488 -0.67006227 -0.66997233 -0.67009053 -0.67017418 -0.66999507\n",
      " -0.67001123 -0.66995441 -0.67011068 -0.67003706 -0.67010233 -0.66997108\n",
      " -0.67002355 -0.67026143 -0.66976549 -0.67000918 -0.67021172 -0.67010305\n",
      " -0.6701708  -0.6701725  -0.67000779 -0.67006705 -0.67003835 -0.67001966\n",
      " -0.67007156 -0.67017314 -0.67000977]\n",
      "-0.6699192199286648\n",
      "rewards6\n",
      "[-0.66974373 -0.66994856 -0.67001739 -0.66985064 -0.66996138 -0.67006291\n",
      " -0.67006061 -0.66984339 -0.66997036 -0.6698692  -0.66987093 -0.66986702\n",
      " -0.6701103  -0.67001462 -0.66980954 -0.66989354 -0.66987668 -0.66983729\n",
      " -0.66983212 -0.66989355 -0.66985194 -0.66993304 -0.66975404 -0.66987897\n",
      " -0.66988674 -0.66993336 -0.67004899 -0.67003683 -0.66986853 -0.66996969\n",
      " -0.66984702 -0.66978474 -0.66991032 -0.66997663 -0.6699187  -0.66991574\n",
      " -0.66973466 -0.66982835 -0.66999305 -0.67005895 -0.66998042 -0.67001426\n",
      " -0.66994782 -0.67000249 -0.66986157 -0.67005628 -0.66990781 -0.67000635\n",
      " -0.66989228 -0.66991612 -0.66985196 -0.66994559 -0.67014505 -0.6698912\n",
      " -0.66986634 -0.66996646 -0.66986975 -0.66992688 -0.67001579 -0.6698484\n",
      " -0.66994379 -0.66996419 -0.67022393]\n",
      "-0.6697769283254341\n",
      "rewards7\n",
      "[-0.66974947 -0.66978102 -0.66992628 -0.66974797 -0.66974684 -0.66982153\n",
      " -0.66968282 -0.66986785 -0.66985678 -0.66996566 -0.66963096 -0.66984795\n",
      " -0.66984247 -0.66982814 -0.66981366 -0.66993502 -0.66970769 -0.66995678\n",
      " -0.66975187 -0.66963725 -0.66966089 -0.6699781  -0.66991203 -0.66972339\n",
      " -0.66983818 -0.66995364 -0.66973708 -0.66966446 -0.66992629 -0.66993215\n",
      " -0.66953997 -0.66964094 -0.66961419 -0.669756   -0.66946284 -0.66976608\n",
      " -0.66972234 -0.66987351 -0.66985745 -0.66976613 -0.66987242 -0.66982206\n",
      " -0.66958207 -0.66966929 -0.66983347 -0.66984071 -0.66982789 -0.6698031\n",
      " -0.66990206 -0.66968304 -0.66957456 -0.66964114 -0.67001319 -0.66996976\n",
      " -0.66971773 -0.66987994 -0.6698087  -0.66993269 -0.66976057 -0.66970075\n",
      " -0.66971577 -0.66978892 -0.66964699]\n",
      "-0.669568765507002\n",
      "rewards8\n",
      "[-0.66958652 -0.66960236 -0.66953191 -0.66956543 -0.66940618 -0.66956831\n",
      " -0.66953016 -0.66952516 -0.66956471 -0.66945682 -0.66952124 -0.66964106\n",
      " -0.66959255 -0.6694736  -0.66946902 -0.66955158 -0.66947793 -0.66960227\n",
      " -0.66949635 -0.66954531 -0.6694684  -0.66958267 -0.66951378 -0.6696199\n",
      " -0.6697448  -0.66956282 -0.66951341 -0.6694585  -0.66948954 -0.66962363\n",
      " -0.66969623 -0.66955007 -0.66950636 -0.66961108 -0.66950708 -0.66960166\n",
      " -0.66965617 -0.66975431 -0.66962411 -0.66958181 -0.66956401 -0.66959156\n",
      " -0.66958696 -0.66962325 -0.66961102 -0.66963019 -0.66952387 -0.66962203\n",
      " -0.66955612 -0.66962613 -0.6695909  -0.66952366 -0.66963009 -0.66960304\n",
      " -0.6695938  -0.66985002 -0.66951612 -0.66962538 -0.66940871 -0.66954386\n",
      " -0.66949718 -0.66960977 -0.66957058]\n",
      "-0.6694445641220148\n",
      "rewards9\n",
      "[-0.66940813 -0.6693122  -0.66934893 -0.66949934 -0.66934339 -0.6695099\n",
      " -0.66954083 -0.66940861 -0.66950734 -0.66947858 -0.66958656 -0.66965864\n",
      " -0.66937118 -0.66936867 -0.66953978 -0.66950025 -0.66960698 -0.6694152\n",
      " -0.66954674 -0.6693425  -0.66943193 -0.66933905 -0.669347   -0.66938304\n",
      " -0.66926547 -0.66934245 -0.66939016 -0.66931811 -0.66936582 -0.6695284\n",
      " -0.66941368 -0.66928598 -0.66937902 -0.66928805 -0.66952708 -0.66930769\n",
      " -0.66940218 -0.66947727 -0.66953543 -0.6696424  -0.66946484 -0.66922165\n",
      " -0.66935084 -0.66959933 -0.66951992 -0.66927484 -0.66957355 -0.66956932\n",
      " -0.66951121 -0.66939152 -0.6695288  -0.66939764 -0.66960559 -0.66951258\n",
      " -0.66939665 -0.66958251 -0.66931478 -0.66942064 -0.66943278 -0.66946834\n",
      " -0.66924674 -0.66951484 -0.66941691]\n",
      "-0.6692637018966938\n",
      "rewards10\n",
      "[-0.66914484 -0.66923348 -0.66918334 -0.66938843 -0.6692395  -0.66931841\n",
      " -0.6694097  -0.66921483 -0.66935882 -0.66943176 -0.66928441 -0.66923045\n",
      " -0.66924909 -0.66915149 -0.6693561  -0.66931842 -0.66917196 -0.66930446\n",
      " -0.66914252 -0.66909919 -0.66930131 -0.66921534 -0.66906889 -0.66925231\n",
      " -0.66913232 -0.66941525 -0.66914477 -0.66937342 -0.66936162 -0.66913811\n",
      " -0.66921501 -0.66944788 -0.66908509 -0.66952218 -0.66921396 -0.66949052\n",
      " -0.66909141 -0.66910281 -0.66916973 -0.66929364 -0.66924893 -0.66930794\n",
      " -0.669177   -0.66944122 -0.66918984 -0.66901444 -0.66914017 -0.66916625\n",
      " -0.66920088 -0.66918563 -0.66920636 -0.6692283  -0.66917735 -0.66927775\n",
      " -0.66928748 -0.66936853 -0.66918528 -0.66928546 -0.66932958 -0.66921511\n",
      " -0.66914503 -0.66913522 -0.66909615]\n",
      "-0.669075238749831\n",
      "rewards11\n",
      "[-0.66920059 -0.66909067 -0.66919033 -0.66907535 -0.66904056 -0.66908711\n",
      " -0.66925932 -0.66900158 -0.6690643  -0.66912346 -0.66925032 -0.6690732\n",
      " -0.66901496 -0.66912994 -0.66896357 -0.66896503 -0.66912267 -0.6691084\n",
      " -0.66926946 -0.66916882 -0.66902687 -0.66908649 -0.66906349 -0.66913848\n",
      " -0.66913395 -0.66922528 -0.6691258  -0.66906217 -0.66909785 -0.66897792\n",
      " -0.66904261 -0.66897445 -0.66892801 -0.66917775 -0.6690814  -0.66913593\n",
      " -0.66901263 -0.66908974 -0.66885569 -0.66929006 -0.66918018 -0.66911584\n",
      " -0.668949   -0.66919539 -0.66910098 -0.6689508  -0.66899083 -0.66896995\n",
      " -0.66911474 -0.6692046  -0.66906219 -0.66890038 -0.66900538 -0.66913315\n",
      " -0.6691019  -0.66916979 -0.6690078  -0.66921275 -0.66914786 -0.66919238\n",
      " -0.66919271 -0.66895782 -0.66908069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6689242181387973\n",
      "rewards12\n",
      "[-0.66895017 -0.66885992 -0.66882149 -0.66890942 -0.66886418 -0.66875842\n",
      " -0.66883676 -0.66901484 -0.66889618 -0.66885755 -0.66887827 -0.66893405\n",
      " -0.66895848 -0.66894261 -0.6689124  -0.66894475 -0.66900333 -0.6687021\n",
      " -0.66884329 -0.66889822 -0.66875179 -0.66906811 -0.66864104 -0.66895462\n",
      " -0.6689725  -0.66874874 -0.66892099 -0.66904643 -0.66900949 -0.66891246\n",
      " -0.66918017 -0.66880613 -0.66891715 -0.66891423 -0.66893488 -0.66912445\n",
      " -0.66905521 -0.66893521 -0.66889058 -0.66891668 -0.66896457 -0.66888164\n",
      " -0.6691148  -0.66893961 -0.66895976 -0.66901831 -0.66874711 -0.66897904\n",
      " -0.66877493 -0.66892174 -0.66903521 -0.66898815 -0.66899214 -0.66891213\n",
      " -0.66902654 -0.66893628 -0.66882995 -0.66902319 -0.66913765 -0.66891102\n",
      " -0.66893087 -0.66902939 -0.66897968]\n",
      "-0.6687250837596148\n",
      "rewards13\n",
      "[-0.66872155 -0.66854073 -0.66848645 -0.66859351 -0.66890718 -0.6688327\n",
      " -0.66877985 -0.6688279  -0.66879779 -0.66845962 -0.66865137 -0.66863164\n",
      " -0.66885529 -0.66871811 -0.66857832 -0.6687972  -0.66862301 -0.66879681\n",
      " -0.6685589  -0.66854504 -0.66855063 -0.66867488 -0.66865902 -0.66885356\n",
      " -0.66888894 -0.66875343 -0.66874399 -0.66867281 -0.66886427 -0.66877212\n",
      " -0.66864835 -0.66879316 -0.66872379 -0.66865647 -0.66861493 -0.66842559\n",
      " -0.66862336 -0.6688075  -0.66871542 -0.66893111 -0.66849631 -0.66865381\n",
      " -0.66882295 -0.66861506 -0.66878185 -0.66873699 -0.66875809 -0.66867216\n",
      " -0.66887862 -0.66858281 -0.66879503 -0.66874131 -0.66883189 -0.66887646\n",
      " -0.66869983 -0.66857835 -0.66871803 -0.66850297 -0.66858299 -0.66872059\n",
      " -0.66861852 -0.66881427 -0.6687163 ]\n",
      "-0.6684858458324234\n"
     ]
    }
   ],
   "source": [
    "#Execute the cross entrophy method with default Values\n",
    "#scores = cem()\n",
    "\n",
    "\n",
    "#To don't ask the GPU as much reduce the pop_size, it's the amount of elemts try\n",
    "scores = cem_no_net()\n",
    "# \n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the weights from file\n",
    "# Not working know\n",
    "\n",
    "\n",
    "#state = env.reset()\n",
    "env = env_pybullet_kin_gen3()\n",
    "env.update_parameters_to_modify([\"mass\",\"max_vel\",\"kp\",\"ki\",\"damping\",\"force_x_one\",\"Ixx\",\"Iyy\",\"Izz\"])\n",
    "env.robot.visual_inspection = False\n",
    "env.modified_parameters_df = env.create_df_from_Excel(\"./Parameters_train_tcp_euc.xlsx\")\n",
    "\n",
    "\n",
    "t.sleep(0.02)\n",
    "action = env.action_modified()\n",
    "action = np.array(action)\n",
    "print(action)\n",
    "reward = env.step_tcp_rishabh(action)\n",
    "\n",
    "print(\"reward\")\n",
    "print(reward)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to excel\n",
    "a = env.df_avg.to_numpy()\n",
    "print(a[:,5])\n",
    "env.df_avg.to_excel(\"./Train_parameters_result_tcp_euc.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.original_parameters_df"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
